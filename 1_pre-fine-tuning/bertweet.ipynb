{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3v1B5dKD7Y8Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (5.2.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-09 16:05:40.856399: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-09 16:05:41.309664: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-09 16:05:41.309721: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-09 16:05:41.344578: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-09 16:05:41.419686: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-09 16:05:43.038884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import TFRobertaForSequenceClassification, AutoTokenizer\n",
    "seed_value = 29\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "np.set_printoptions(precision=2)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_addons as tfa\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder as ohe\n",
    "from sklearn.metrics import auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 16:05:46.313744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22287 MB memory:  -> device: 6, name: NVIDIA A30, pci bus id: 0000:c1:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[6], 'GPU')\n",
    "logical_devices = tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BMKbWyXT7kb4"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 200\n",
    "BERT = 'vinai/bertweet-large'\n",
    "N_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NQwWopB77pr1"
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    print(f'reading {path}')\n",
    "    data = pd.read_csv(path)\n",
    "    data.text = data.apply(lambda row: row.text.encode('ascii', 'ignore').decode('ascii').lower(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"removed|deleted\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\" :\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[a-zA-Z]*lt;3[a-zA-Z]*\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[a-zA-Z]&[a-zA-Z]*\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[^a-zA-Z:.,;'!?\\d]+\", \" \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"i m |im |i'm \", \"i am \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"ive \", \"i have \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"wasnt|wasn't\", \"was not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"werent|weren't\", \"were not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"dont|don't\", \"do not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"doesnt|doesn't\", \"does not\", row.text).strip(), 1)\n",
    "    texts = data.text.values\n",
    "    labels = data.labels.values\n",
    "    encoder = ohe(sparse=False)\n",
    "    labels = np.array(labels).reshape(-1, 1)\n",
    "    enc_labels = encoder.fit_transform(labels)\n",
    "    print(f'texts shape: {texts.shape}, labels shape: {enc_labels.shape}')\n",
    "    return texts, enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1YkmmDUT8Vga"
   },
   "outputs": [],
   "source": [
    "def prepare_bert_input(sentences, seq_len, bert_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "    encodings = tokenizer(sentences.tolist(), truncation=True, padding='max_length', max_length=seq_len)\n",
    "    if bert_name.startswith(\"roberta\") or \"bertweet\" in bert_name or \"distilbert\" in bert_name:\n",
    "        input = [np.array(encodings[\"input_ids\"]), np.array(encodings[\"attention_mask\"])]\n",
    "    else:\n",
    "        input = [np.array(encodings[\"input_ids\"]), np.array(encodings[\"attention_mask\"]),\n",
    "               np.array(encodings[\"token_type_ids\"])]\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10952,
     "status": "ok",
     "timestamp": 1701817510390,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "i-4Je75_8bNl",
    "outputId": "808f84a8-99e1-4607-82e1-9e030004826d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ../dep-det-data/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts shape: (6006,), labels shape: (6006, 3)\n",
      "reading ../dep-det-data/dev.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts shape: (1000,), labels shape: (1000, 3)\n",
      "reading ../dep-det-data/test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts shape: (3245,), labels shape: (3245, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 614/614 [00:00<00:00, 1.92MB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 2.01MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.36MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.47MB/s]\n"
     ]
    }
   ],
   "source": [
    "sentences_train, labels_train = read_data(\"../dep-det-data/train.csv\")\n",
    "sentences_val, labels_val = read_data(\"../dep-det-data/dev.csv\")\n",
    "sentences_test, labels_test = read_data(\"../dep-det-data/test.csv\")\n",
    "\n",
    "# permutation train\n",
    "perm_train = np.random.permutation(len(sentences_train))\n",
    "sentences_train = sentences_train[perm_train]\n",
    "labels_train = labels_train[perm_train]\n",
    "\n",
    "# permutation val\n",
    "perm_val = np.random.permutation(len(sentences_val))\n",
    "sentences_val = sentences_val[perm_val]\n",
    "labels_val = labels_val[perm_val]\n",
    "\n",
    "# permutation test\n",
    "perm_test = np.random.permutation(len(sentences_test))\n",
    "sentences_test = sentences_test[perm_test]\n",
    "labels_test = labels_test[perm_test]\n",
    "\n",
    "# prepare model input\n",
    "X_train = prepare_bert_input(sentences_train, MAX_SEQ_LEN, BERT)\n",
    "X_val = prepare_bert_input(sentences_val, MAX_SEQ_LEN, BERT)\n",
    "X_test = prepare_bert_input(sentences_test, MAX_SEQ_LEN, BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 292873,
     "status": "error",
     "timestamp": 1701820263972,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "Ez0L7WSm8cNR",
    "outputId": "31aec781-35ea-4c24-ef06-525f38a0ce46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf_model.h5: 100%|██████████| 1.63G/1.63G [01:54<00:00, 14.2MB/s]\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " roberta_input_ids (InputLa  [(None, 200)]                0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " roberta_attention_mask (In  [(None, 200)]                0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " tf_roberta_for_sequence_cl  TFSequenceClassifierOutput   3553628   ['roberta_input_ids[0][0]',   \n",
      " assification (TFRobertaFor  (loss=None, logits=(None,    19         'roberta_attention_mask[0][0]\n",
      " SequenceClassification)     3),                                    ']                            \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None)                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 355362819 (1.32 GB)\n",
      "Trainable params: 355362819 (1.32 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Bertweet initialization for pre-fine-tuning (it builds upon a Roberta model. See HuggingFace docs.)\n",
    "roberta_input_ids = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_input_ids')\n",
    "roberta_input_mask = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_attention_mask')\n",
    "roberta_inputs = [roberta_input_ids, roberta_input_mask]\n",
    "roberta = TFRobertaForSequenceClassification.from_pretrained(BERT, num_labels=N_CLASSES)\n",
    "roberta_output = roberta(roberta_inputs).logits\n",
    "pre_finetuned_model = keras.Model(inputs=roberta_inputs, outputs=roberta_output)\n",
    "pre_finetuned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " roberta_input_ids (InputLa  [(None, 200)]                0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " roberta_attention_mask (In  [(None, 200)]                0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " tf_roberta_for_sequence_cl  TFSequenceClassifierOutput   3553628   ['roberta_input_ids[0][0]',   \n",
      " assification (TFRobertaFor  (loss=None, logits=(None,    19         'roberta_attention_mask[0][0]\n",
      " SequenceClassification)     3),                                    ']                            \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None)                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 355362819 (1.32 GB)\n",
      "Trainable params: 355362819 (1.32 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 6\n",
    "batch_size = 16\n",
    "\n",
    "opt = tfa.optimizers.RectifiedAdam(learning_rate=3e-5)\n",
    "loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "best_weights_file = f\"bertweet_preft.h5\"\n",
    "acc = keras.metrics.CategoricalAccuracy()\n",
    "f1_macro = keras.metrics.F1Score(average='macro')\n",
    "m_ckpt = ModelCheckpoint(best_weights_file, monitor='val_'+f1_macro.name, mode='max', verbose=2,\n",
    "                          save_weights_only=True, save_best_only=True)\n",
    "\n",
    "pre_finetuned_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "pre_finetuned_model.summary()\n",
    "\n",
    "### uncomment to perform the pre-fine-tuning step ###\n",
    "\n",
    "# pre_finetuned_model.fit(\n",
    "# X_train, labels_train,\n",
    "# validation_data=(X_val, labels_val),\n",
    "# epochs=max_epochs,\n",
    "# batch_size=batch_size,\n",
    "# callbacks=[m_ckpt],\n",
    "# verbose=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 16:08:09.403288: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 66s 568ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.435     0.539     0.481       228\n",
      "           1      0.750     0.772     0.761      2169\n",
      "           2      0.491     0.423     0.455       848\n",
      "\n",
      "    accuracy                          0.664      3245\n",
      "   macro avg      0.559     0.578     0.566      3245\n",
      "weighted avg      0.660     0.664     0.661      3245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "from sklearn.metrics import classification_report\n",
    "best_weights_file = f\"bertweet.h5\"\n",
    "pre_finetuned_model.load_weights(best_weights_file)\n",
    "pre_finetuned_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "y_pred_probs = pre_finetuned_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "labels_test_decode = np.argmax(labels_test, axis=1)\n",
    "\n",
    "report = classification_report(labels_test_decode, y_pred, digits=3)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPwYJoECmZY3TsCeI3Ln81/",
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
