{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3v1B5dKD7Y8Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import TFRobertaForSequenceClassification, AutoTokenizer\n",
    "seed_value = 29\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "np.set_printoptions(precision=2)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_addons as tfa\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder as ohe\n",
    "from sklearn.metrics import auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.set_visible_devices(physical_devices[6], 'GPU')\n",
    "# logical_devices = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "logical_devices = tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BMKbWyXT7kb4"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 200\n",
    "BERT = 'vinai/bertweet-large'\n",
    "N_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NQwWopB77pr1"
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    print(f'reading {path}')\n",
    "    data = pd.read_csv(path)\n",
    "    data.text = data.apply(lambda row: row.text.encode('ascii', 'ignore').decode('ascii').lower(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"removed|deleted\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\" :\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[a-zA-Z]*lt;3[a-zA-Z]*\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[a-zA-Z]&[a-zA-Z]*\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[^a-zA-Z:.,;'!?\\d]+\", \" \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"i m |im |i'm \", \"i am \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"ive \", \"i have \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"wasnt|wasn't\", \"was not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"werent|weren't\", \"were not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"dont|don't\", \"do not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"doesnt|doesn't\", \"does not\", row.text).strip(), 1)\n",
    "    texts = data.text.values\n",
    "    labels = data.labels.values\n",
    "    encoder = ohe(sparse=False)\n",
    "    labels = np.array(labels).reshape(-1, 1)\n",
    "    enc_labels = encoder.fit_transform(labels)\n",
    "    print(f'texts shape: {texts.shape}, labels shape: {enc_labels.shape}')\n",
    "    return texts, enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1YkmmDUT8Vga"
   },
   "outputs": [],
   "source": [
    "def prepare_bert_input(sentences, seq_len, bert_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "    encodings = tokenizer(sentences.tolist(), truncation=True, padding='max_length', max_length=seq_len)\n",
    "    if bert_name.startswith(\"roberta\") or \"bertweet\" in bert_name or \"distilbert\" in bert_name:\n",
    "        input = [np.array(encodings[\"input_ids\"]), np.array(encodings[\"attention_mask\"])]\n",
    "    else:\n",
    "        input = [np.array(encodings[\"input_ids\"]), np.array(encodings[\"attention_mask\"]),\n",
    "               np.array(encodings[\"token_type_ids\"])]\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10952,
     "status": "ok",
     "timestamp": 1701817510390,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "i-4Je75_8bNl",
    "outputId": "808f84a8-99e1-4607-82e1-9e030004826d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# def read_data(file_path):\n",
    "#     # Read the CSV file\n",
    "#     data = pd.read_csv(file_path)\n",
    "    \n",
    "#     # Extract texts and labels\n",
    "#     texts = data['text'].values\n",
    "#     labels = data['labels'].values\n",
    "    \n",
    "#     # Initialize the OneHotEncoder with the correct parameter\n",
    "#     encoder = OneHotEncoder(sparse_output=False)\n",
    "    \n",
    "#     # Reshape labels and apply one-hot encoding\n",
    "#     labels = np.array(labels).reshape(-1, 1)\n",
    "#     enc_labels = encoder.fit_transform(labels)\n",
    "    \n",
    "#     return texts, enc_labels\n",
    "\n",
    "sentences_train, labels_train = read_data(\"../dep-det-data/train.csv\")\n",
    "sentences_val, labels_val = read_data(\"../dep-det-data/dev.csv\")\n",
    "sentences_test, labels_test = read_data(\"../dep-det-data/test.csv\")\n",
    "\n",
    "# sentences_train, labels_train = read_data(\"train.csv\")\n",
    "# sentences_val, labels_val = read_data(\"dev.csv\")\n",
    "# sentences_test, labels_test = read_data(\"test.csv\")\n",
    "\n",
    "# permutation train\n",
    "perm_train = np.random.permutation(len(sentences_train))\n",
    "sentences_train = sentences_train[perm_train]\n",
    "labels_train = labels_train[perm_train]\n",
    "\n",
    "# permutation val\n",
    "perm_val = np.random.permutation(len(sentences_val))\n",
    "sentences_val = sentences_val[perm_val]\n",
    "labels_val = labels_val[perm_val]\n",
    "\n",
    "# permutation test\n",
    "perm_test = np.random.permutation(len(sentences_test))\n",
    "sentences_test = sentences_test[perm_test]\n",
    "labels_test = labels_test[perm_test]\n",
    "\n",
    "# prepare model input\n",
    "X_train = prepare_bert_input(sentences_train, MAX_SEQ_LEN, BERT)\n",
    "X_val = prepare_bert_input(sentences_val, MAX_SEQ_LEN, BERT)\n",
    "X_test = prepare_bert_input(sentences_test, MAX_SEQ_LEN, BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 292873,
     "status": "error",
     "timestamp": 1701820263972,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "Ez0L7WSm8cNR",
    "outputId": "31aec781-35ea-4c24-ef06-525f38a0ce46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " roberta_input_ids (InputLa  [(None, 200)]                0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " roberta_attention_mask (In  [(None, 200)]                0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " tf_roberta_for_sequence_cl  TFSequenceClassifierOutput   3553628   ['roberta_input_ids[0][0]',   \n",
      " assification (TFRobertaFor  (loss=None, logits=(None,    19         'roberta_attention_mask[0][0]\n",
      " SequenceClassification)     3),                                    ']                            \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None)                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 355362819 (1.32 GB)\n",
      "Trainable params: 355362819 (1.32 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Bertweet initialization for pre-fine-tuning (it builds upon a Roberta model. See HuggingFace docs.)\n",
    "roberta_input_ids = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_input_ids')\n",
    "roberta_input_mask = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_attention_mask')\n",
    "roberta_inputs = [roberta_input_ids, roberta_input_mask]\n",
    "roberta = TFRobertaForSequenceClassification.from_pretrained(BERT, num_labels=N_CLASSES)\n",
    "roberta_output = roberta(roberta_inputs).logits\n",
    "pre_finetuned_model = keras.Model(inputs=roberta_inputs, outputs=roberta_output)\n",
    "pre_finetuned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " roberta_input_ids (InputLa  [(None, 200)]                0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " roberta_attention_mask (In  [(None, 200)]                0         []                            \n",
      " putLayer)                                                                                        \n",
      "                                                                                                  \n",
      " tf_roberta_for_sequence_cl  TFSequenceClassifierOutput   3553628   ['roberta_input_ids[0][0]',   \n",
      " assification (TFRobertaFor  (loss=None, logits=(None,    19         'roberta_attention_mask[0][0]\n",
      " SequenceClassification)     3),                                    ']                            \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None)                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 355362819 (1.32 GB)\n",
      "Trainable params: 355362819 (1.32 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 6\n",
    "batch_size = 16\n",
    "\n",
    "opt = tfa.optimizers.RectifiedAdam(learning_rate=3e-5)\n",
    "loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "best_weights_file = f\"bertweet_preft.h5\"\n",
    "acc = keras.metrics.CategoricalAccuracy()\n",
    "f1_macro = keras.metrics.F1Score(average='macro')\n",
    "m_ckpt = ModelCheckpoint(best_weights_file, monitor='val_'+f1_macro.name, mode='max', verbose=2,\n",
    "                          save_weights_only=True, save_best_only=True)\n",
    "\n",
    "pre_finetuned_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "pre_finetuned_model.summary()\n",
    "\n",
    "### uncomment to perform the pre-fine-tuning step ###\n",
    "\n",
    "# pre_finetuned_model.fit(\n",
    "# X_train, labels_train,\n",
    "# validation_data=(X_val, labels_val),\n",
    "# epochs=max_epochs,\n",
    "# batch_size=batch_size,\n",
    "# callbacks=[m_ckpt],\n",
    "# verbose=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'bertweet.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m      3\u001b[0m best_weights_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbertweet.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mpre_finetuned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_weights_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m pre_finetuned_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mloss, optimizer\u001b[38;5;241m=\u001b[39mopt, metrics\u001b[38;5;241m=\u001b[39m[f1_macro,acc])\n\u001b[0;32m      6\u001b[0m y_pred_probs \u001b[38;5;241m=\u001b[39m pre_finetuned_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'bertweet.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "from sklearn.metrics import classification_report\n",
    "best_weights_file = f\"bertweet.h5\"\n",
    "pre_finetuned_model.load_weights(best_weights_file)\n",
    "pre_finetuned_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "y_pred_probs = pre_finetuned_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "labels_test_decode = np.argmax(labels_test, axis=1)\n",
    "\n",
    "report = classification_report(labels_test_decode, y_pred, digits=3)\n",
    "print(report)\n",
    "\n",
    "# if os.path.isfile(best_weights_file):\n",
    "#     pre_finetuned_model.load_weights(best_weights_file)\n",
    "#     pre_finetuned_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "#     y_pred_probs = pre_finetuned_model.predict(X_test)\n",
    "#     y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "#     labels_test_decode = np.argmax(labels_test, axis=1)\n",
    "\n",
    "#     report = classification_report(labels_test_decode, y_pred, digits=3)\n",
    "#     print(report)\n",
    "# else:\n",
    "#     print(f\"The file {best_weights_file} does not exist.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPwYJoECmZY3TsCeI3Ln81/",
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
