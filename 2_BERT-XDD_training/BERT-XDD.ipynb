{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3v1B5dKD7Y8Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import TFRobertaForSequenceClassification, AutoTokenizer\n",
    "seed_value = 29\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "np.set_printoptions(precision=2)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_addons as tfa\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder as ohe\n",
    "from sklearn.metrics import auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "logical_devices = tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BMKbWyXT7kb4"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 200\n",
    "BERT = 'vinai/bertweet-large'\n",
    "N_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NQwWopB77pr1"
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    print(f'reading {path}')\n",
    "    data = pd.read_csv(path)\n",
    "    data.text = data.apply(lambda row: row.text.encode('ascii', 'ignore').decode('ascii').lower(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"removed|deleted\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\" :\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[a-zA-Z]*lt;3[a-zA-Z]*\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[a-zA-Z]&[a-zA-Z]*\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[^a-zA-Z:.,;'!?\\d]+\", \" \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"i m |im |i'm \", \"i am \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"ive \", \"i have \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"wasnt|wasn't\", \"was not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"werent|weren't\", \"were not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"dont|don't\", \"do not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"doesnt|doesn't\", \"does not\", row.text).strip(), 1)\n",
    "    texts = data.text.values\n",
    "    labels = data.labels.values\n",
    "    encoder = ohe(sparse=False)\n",
    "    labels = np.array(labels).reshape(-1, 1)\n",
    "    enc_labels = encoder.fit_transform(labels)\n",
    "    print(f'texts shape: {texts.shape}, labels shape: {enc_labels.shape}')\n",
    "    return texts, enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1YkmmDUT8Vga"
   },
   "outputs": [],
   "source": [
    "def prepare_bert_input(sentences, seq_len, bert_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "    encodings = tokenizer(sentences.tolist(), truncation=True, padding='max_length', max_length=seq_len)\n",
    "    if bert_name.startswith(\"roberta\") or \"bertweet\" in bert_name or \"distilbert\" in bert_name:\n",
    "        input = [np.array(encodings[\"input_ids\"]), np.array(encodings[\"attention_mask\"])]\n",
    "    else:\n",
    "        input = [np.array(encodings[\"input_ids\"]), np.array(encodings[\"attention_mask\"]),\n",
    "               np.array(encodings[\"token_type_ids\"])]\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = English()\n",
    "\n",
    "def compute_att_mask(ids):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT)\n",
    "    no_sw_att_mask = []\n",
    "    for enc_sentence in ids:\n",
    "        tokens = [tokenizer.decode([i]) for i in enc_sentence]\n",
    "        lexemes = [nlp.vocab[token.strip()] for token in tokens]\n",
    "        mask = [1 if not (lexeme.is_stop or lexeme.is_punct or len(lexeme.text) <= 2 or \"<\" in lexeme.text) else 0 for lexeme in lexemes]\n",
    "        no_sw_att_mask.append(np.array(mask))\n",
    "    return np.array(no_sw_att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10952,
     "status": "ok",
     "timestamp": 1701817510390,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "i-4Je75_8bNl",
    "outputId": "808f84a8-99e1-4607-82e1-9e030004826d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def read_data(file_path):\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    texts = data['text'].values\n",
    "    labels = data['labels'].values\n",
    "    \n",
    "    # Initialize the OneHotEncoder with the correct parameter\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    \n",
    "    # Reshape labels and apply one-hot encoding\n",
    "    labels = np.array(labels).reshape(-1, 1)\n",
    "    enc_labels = encoder.fit_transform(labels)\n",
    "    \n",
    "    return texts, enc_labels\n",
    "\n",
    "sentences_train, labels_train = read_data(\"../dep-det-data/train.csv\")\n",
    "sentences_val, labels_val = read_data(\"../dep-det-data/dev.csv\")\n",
    "sentences_test, labels_test = read_data(\"../dep-det-data/test.csv\")\n",
    "\n",
    "# permutation train\n",
    "perm_train = np.random.permutation(len(sentences_train))\n",
    "sentences_train = sentences_train[perm_train]\n",
    "labels_train = labels_train[perm_train]\n",
    "\n",
    "# permutation val\n",
    "perm_val = np.random.permutation(len(sentences_val))\n",
    "sentences_val = sentences_val[perm_val]\n",
    "labels_val = labels_val[perm_val]\n",
    "\n",
    "# permutation test\n",
    "perm_test = np.random.permutation(len(sentences_test))\n",
    "sentences_test = sentences_test[perm_test]\n",
    "labels_test = labels_test[perm_test]\n",
    "\n",
    "# prepare model input\n",
    "X_train = prepare_bert_input(sentences_train, MAX_SEQ_LEN, BERT)\n",
    "X_val = prepare_bert_input(sentences_val, MAX_SEQ_LEN, BERT)\n",
    "X_test = prepare_bert_input(sentences_test, MAX_SEQ_LEN, BERT)\n",
    "\n",
    "# add custom attention mask\n",
    "mask_train = compute_att_mask(X_train[0])\n",
    "X_train = [X_train[0], X_train[1], mask_train]\n",
    "mask_val = compute_att_mask(X_val[0])\n",
    "X_val = [X_val[0], X_val[1], mask_val]\n",
    "mask_test = compute_att_mask(X_test[0])\n",
    "X_test = [X_test[0], X_test[1], mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 292873,
     "status": "error",
     "timestamp": 1701820263972,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "Ez0L7WSm8cNR",
    "outputId": "31aec781-35ea-4c24-ef06-525f38a0ce46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " roberta_model_input_ids (I  [(None, 200)]                0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " roberta_model_attention_ma  [(None, 200)]                0         []                            \n",
      " sk (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   3543101   ['roberta_model_input_ids[0][0\n",
      " r)                          ngAndCrossAttentions(last_   44        ]',                           \n",
      "                             hidden_state=(None, 200, 1              'roberta_model_attention_mask\n",
      "                             024),                                  [0][0]']                      \n",
      "                              pooler_output=None, past_                                           \n",
      "                             key_values=None, hidden_st                                           \n",
      "                             ates=None, attentions=None                                           \n",
      "                             , cross_attentions=None)                                             \n",
      "                                                                                                  \n",
      " custom_att_mask (InputLaye  [(None, 200)]                0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  [(None, 200, 512),           2623488   ['roberta[0][0]']             \n",
      " al)                          (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " tf.ones_like (TFOpLambda)   (None, 200)                  0         ['custom_att_mask[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 200, 512)             262144    ['bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " subtract (Subtract)         (None, 200)                  0         ['custom_att_mask[0][0]',     \n",
      "                                                                     'tf.ones_like[0][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 200, 1)               512       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 200)                  0         ['subtract[0][0]']            \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 200)                  0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)        (None, 200)                  0         ['tf.math.multiply[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 200)                  0         ['flatten[0][0]',             \n",
      "                                                                     'tf.cast[0][0]']             \n",
      "                                                                                                  \n",
      " alpha (Softmax)             (None, 200)                  0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                   (None, 1024)                 0         ['roberta[0][0]',             \n",
      "                                                                     'alpha[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 3)                    3075      ['dot[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 357199363 (1.33 GB)\n",
      "Trainable params: 357199363 (1.33 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BERT-XDD model initialization\n",
    "roberta_model_input_ids = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_model_input_ids')\n",
    "roberta_model_input_mask = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_model_attention_mask')\n",
    "custom_att_mask = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='custom_att_mask')\n",
    "roberta_model_inputs = [roberta_model_input_ids, roberta_model_input_mask]\n",
    "roberta_model = TFRobertaForSequenceClassification.from_pretrained(BERT, num_labels=N_CLASSES)\n",
    "roberta_model_encoder, roberta_model_classifier, config = roberta_model.roberta, roberta_model.classifier, roberta_model.config\n",
    "\n",
    "encoder_output = roberta_model_encoder(roberta_model_inputs)\n",
    "hidden_state = encoder_output[0]\n",
    "\n",
    "units=256\n",
    "\n",
    "states, forward_h, _, backward_h, _ = layers.Bidirectional(layers.LSTM(units, return_sequences=True, return_state=True))(hidden_state)\n",
    "hidden = layers.Dense(units*2, activation=\"tanh\", use_bias=False)(states)\n",
    "out = layers.Dense(1, activation='linear', use_bias=False)(hidden)\n",
    "energy = layers.Flatten()(out)\n",
    "ones = tf.ones_like(custom_att_mask)\n",
    "att_mask = layers.Subtract()([custom_att_mask, ones])\n",
    "att_mask = att_mask*10000\n",
    "att_mask = tf.cast(att_mask, \"float32\")\n",
    "flat = layers.Add()([energy, att_mask])\n",
    "normalize = layers.Softmax()\n",
    "normalize._init_set_name(\"alpha\")\n",
    "alpha = normalize(flat)\n",
    "ctx = layers.Dot(axes=1)([hidden_state, alpha])\n",
    "pred = layers.Dense(N_CLASSES, activation=\"softmax\")(ctx)\n",
    "\n",
    "BERT_XDD_model = keras.Model(inputs=[roberta_model_input_ids, roberta_model_input_mask, custom_att_mask], outputs=pred)\n",
    "BERT_XDD_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 292873,
     "status": "error",
     "timestamp": 1701820263972,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "Ez0L7WSm8cNR",
    "outputId": "31aec781-35ea-4c24-ef06-525f38a0ce46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '../1_pre-fine-tuning/bertweet.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m pre_finetuned_roberta_layer \u001b[38;5;241m=\u001b[39m pre_finetuned_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     13\u001b[0m pre_finetuned_encoder, pre_finetuned_classifier \u001b[38;5;241m=\u001b[39m pre_finetuned_roberta_layer\u001b[38;5;241m.\u001b[39mroberta, pre_finetuned_roberta_layer\u001b[38;5;241m.\u001b[39mclassifier\n\u001b[1;32m---> 15\u001b[0m \u001b[43mpre_finetuned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../1_pre-fine-tuning/bertweet.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m BERT_XDD_encoder \u001b[38;5;241m=\u001b[39m BERT_XDD_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     17\u001b[0m BERT_XDD_encoder\u001b[38;5;241m.\u001b[39mset_weights(pre_finetuned_encoder\u001b[38;5;241m.\u001b[39mget_weights())\n",
      "File \u001b[1;32mc:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '../1_pre-fine-tuning/bertweet.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "#load and freeze pre-finetuned encoder's layers\n",
    "\n",
    "# Bertweet pre-finetuned (Roberta architecture)\n",
    "roberta_input_ids = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_input_ids')\n",
    "roberta_input_mask = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_attention_mask')\n",
    "roberta_inputs = [roberta_input_ids, roberta_input_mask]\n",
    "roberta = TFRobertaForSequenceClassification.from_pretrained(BERT, num_labels=N_CLASSES)\n",
    "roberta_output = roberta(roberta_inputs).logits\n",
    "pre_finetuned_model = keras.Model(inputs=roberta_inputs, outputs=roberta_output)\n",
    "\n",
    "pre_finetuned_roberta_input_layer = [pre_finetuned_model.layers[0], pre_finetuned_model.layers[1]]\n",
    "pre_finetuned_roberta_layer = pre_finetuned_model.layers[2]\n",
    "pre_finetuned_encoder, pre_finetuned_classifier = pre_finetuned_roberta_layer.roberta, pre_finetuned_roberta_layer.classifier\n",
    "\n",
    "pre_finetuned_model.load_weights(\"../1_pre-fine-tuning/bertweet.h5\")\n",
    "BERT_XDD_encoder = BERT_XDD_model.layers[2]\n",
    "BERT_XDD_encoder.set_weights(pre_finetuned_encoder.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 292873,
     "status": "error",
     "timestamp": 1701820263972,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "Ez0L7WSm8cNR",
    "outputId": "31aec781-35ea-4c24-ef06-525f38a0ce46"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BERT_XDD_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train the model's head\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mBERT_XDD_encoder\u001b[49m\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      3\u001b[0m BERT_XDD_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m      5\u001b[0m max_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BERT_XDD_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "# train the model's head\n",
    "BERT_XDD_encoder.trainable = False\n",
    "BERT_XDD_model.summary()\n",
    "\n",
    "max_epochs = 6\n",
    "batch_size = 16\n",
    "opt = tf.optimizers.Adam()\n",
    "loss = keras.losses.CategoricalCrossentropy()\n",
    "best_weights_file = f\"BERT-XDD_TL.h5\"\n",
    "acc = keras.metrics.CategoricalAccuracy()\n",
    "f1_macro = keras.metrics.F1Score(average='macro')\n",
    "m_ckpt = ModelCheckpoint(best_weights_file, monitor='val_'+f1_macro.name, mode='max', verbose=2,\n",
    "                          save_weights_only=True, save_best_only=True)\n",
    "\n",
    "BERT_XDD_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "\n",
    "### uncomment to train model's head ###\n",
    "\n",
    "# BERT_XDD_model.fit(\n",
    "# X_train, labels_train,\n",
    "# validation_data=(X_val, labels_val),\n",
    "# epochs=max_epochs,\n",
    "# batch_size=batch_size,\n",
    "# callbacks=[m_ckpt],\n",
    "# verbose=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24891,
     "status": "ok",
     "timestamp": 1701820290757,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "wtVgPnsx8kPN",
    "outputId": "1d1085f5-9737-488b-ae98-781c27fbd6eb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'BERT-XDD_TL.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# test the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m best_weights_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERT-XDD_TL.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mBERT_XDD_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_weights_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m BERT_XDD_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mloss, optimizer\u001b[38;5;241m=\u001b[39mopt, metrics\u001b[38;5;241m=\u001b[39m[f1_macro,acc])\n\u001b[0;32m      9\u001b[0m y_pred_probs \u001b[38;5;241m=\u001b[39m BERT_XDD_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Users\\Ankit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'BERT-XDD_TL.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# test the model\n",
    "\n",
    "best_weights_file = f\"BERT-XDD_TL.h5\"\n",
    "BERT_XDD_model.load_weights(best_weights_file)\n",
    "\n",
    "BERT_XDD_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "y_pred_probs = BERT_XDD_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "labels_test_decode = np.argmax(labels_test, axis=1)\n",
    "report = classification_report(labels_test_decode, y_pred, digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " roberta_model_input_ids (I  [(None, 200)]                0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " roberta_model_attention_ma  [(None, 200)]                0         []                            \n",
      " sk (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   3543101   ['roberta_model_input_ids[0][0\n",
      " r)                          ngAndCrossAttentions(last_   44        ]',                           \n",
      "                             hidden_state=(None, 200, 1              'roberta_model_attention_mask\n",
      "                             024),                                  [0][0]']                      \n",
      "                              pooler_output=None, past_                                           \n",
      "                             key_values=None, hidden_st                                           \n",
      "                             ates=None, attentions=None                                           \n",
      "                             , cross_attentions=None)                                             \n",
      "                                                                                                  \n",
      " custom_att_mask (InputLaye  [(None, 200)]                0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  [(None, 200, 512),           2623488   ['roberta[0][0]']             \n",
      " al)                          (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " tf.ones_like (TFOpLambda)   (None, 200)                  0         ['custom_att_mask[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 200, 512)             262144    ['bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " subtract (Subtract)         (None, 200)                  0         ['custom_att_mask[0][0]',     \n",
      "                                                                     'tf.ones_like[0][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 200, 1)               512       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 200)                  0         ['subtract[0][0]']            \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 200)                  0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)        (None, 200)                  0         ['tf.math.multiply[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 200)                  0         ['flatten[0][0]',             \n",
      "                                                                     'tf.cast[0][0]']             \n",
      "                                                                                                  \n",
      " alpha (Softmax)             (None, 200)                  0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                   (None, 1024)                 0         ['roberta[0][0]',             \n",
      "                                                                     'alpha[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 3)                    3075      ['dot[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 357199363 (1.33 GB)\n",
      "Trainable params: 357199363 (1.33 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## end-to-end fine-tuning\n",
    "\n",
    "best_weights_file = f\"BERT-XDD_TL.h5\"\n",
    "BERT_XDD_model.load_weights(best_weights_file)\n",
    "BERT_XDD_model.trainable = True\n",
    "\n",
    "max_epochs = 2\n",
    "batch_size = 16\n",
    "opt = tfa.optimizers.RectifiedAdam(learning_rate=3e-5)\n",
    "loss = keras.losses.CategoricalCrossentropy()\n",
    "best_weights_file = f\"BERT-XDD_FT.h5\"\n",
    "acc = keras.metrics.CategoricalAccuracy()\n",
    "f1_macro = keras.metrics.F1Score(average='macro')\n",
    "m_ckpt = ModelCheckpoint(best_weights_file, monitor='val_'+f1_macro.name, mode='max', verbose=2,\n",
    "                          save_weights_only=True, save_best_only=True)\n",
    "\n",
    "BERT_XDD_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "BERT_XDD_model.summary()\n",
    "\n",
    "### uncomment to perform the end-to-end fine-tuning step ###\n",
    "\n",
    "# BERT_XDD_model.fit(\n",
    "# X_train, labels_train,\n",
    "# validation_data=(X_val, labels_val),\n",
    "# epochs=max_epochs,\n",
    "# batch_size=batch_size,\n",
    "# callbacks=[m_ckpt],\n",
    "# verbose=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 67s 585ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.498     0.535     0.516       228\n",
      "           1      0.771     0.722     0.745      2169\n",
      "           2      0.478     0.546     0.510       848\n",
      "\n",
      "    accuracy                          0.663      3245\n",
      "   macro avg      0.582     0.601     0.590      3245\n",
      "weighted avg      0.675     0.663     0.668      3245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "from sklearn.metrics import classification_report\n",
    "best_weights_file = f\"BERT-XDD_FT.h5\"\n",
    "BERT_XDD_model.load_weights(best_weights_file)\n",
    "BERT_XDD_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "y_pred_probs = BERT_XDD_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "labels_test_decode = np.argmax(labels_test, axis=1)\n",
    "\n",
    "report = classification_report(labels_test_decode, y_pred, digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 67s 586ms/step\n"
     ]
    }
   ],
   "source": [
    "# create the attention inspection model (for explainability purposes)\n",
    "best_weights_file = f\"BERT-XDD_FT.h5\"\n",
    "BERT_XDD_model.load_weights(best_weights_file)\n",
    "BERT_XDD_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "weight_model = keras.Model(\n",
    "    inputs=[roberta_model_input_ids, roberta_model_input_mask, custom_att_mask],\n",
    "    outputs=BERT_XDD_model.get_layer(\"alpha\").output)\n",
    "# restore weights\n",
    "for l1, l2 in zip(weight_model.layers, BERT_XDD_model.layers):\n",
    "    l1.set_weights(l2.get_weights())\n",
    "weight_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "\n",
    "out_p = weight_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_name(arr):\n",
    "    n = {0:\"SEVERE_DEP\", 1: \"DEP\", 2: \"NORMAL\"}\n",
    "    return n[np.argmax(arr)]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT)\n",
    "def get_word_weights(id, top_n=200):\n",
    "    ids_test = X_test[0]\n",
    "    tokens = [tokenizer.decode([i]) for i in ids_test[id]]\n",
    "    d = {}\n",
    "    for token, weight in zip(tokens, out_p[id]):\n",
    "        weight = weight\n",
    "        if token not in d:\n",
    "            d[token]=weight\n",
    "        else:\n",
    "            d[token] = max(d[token], weight)\n",
    "    d_sorted = dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:top_n])\n",
    "    in_sentence = tokenizer.decode(ids_test[id])\n",
    "    end = in_sentence.index(\"</s>\")\n",
    "    return {\"sentence\": in_sentence[:end],\n",
    "            \"pred_label\":get_label_name(y_pred_probs[id]),\n",
    "            \"real_label\":get_label_name(labels_test[id]),\n",
    "            \"weights\": d_sorted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m): \u001b[38;5;66;03m# example expl\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_word_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_id\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m, in \u001b[0;36mget_word_weights\u001b[1;34m(id, top_n)\u001b[0m\n\u001b[0;32m      8\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode([i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ids_test[\u001b[38;5;28mid\u001b[39m]]\n\u001b[0;32m      9\u001b[0m d \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tokens, \u001b[43mout_p\u001b[49m[\u001b[38;5;28mid\u001b[39m]):\n\u001b[0;32m     11\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m d:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'out_p' is not defined"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for test_id in range(10): # example expl\n",
    "    print(get_word_weights(test_id), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPwYJoECmZY3TsCeI3Ln81/",
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
