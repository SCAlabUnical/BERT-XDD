{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3v1B5dKD7Y8Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (5.2.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-09 17:03:22.587413: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-09 17:03:22.625170: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-09 17:03:22.625196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-09 17:03:22.626132: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-09 17:03:22.632121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-09 17:03:23.722567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import TFRobertaForSequenceClassification, AutoTokenizer\n",
    "seed_value = 29\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "np.set_printoptions(precision=2)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_addons as tfa\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder as ohe\n",
    "from sklearn.metrics import auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 17:03:25.488567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22056 MB memory:  -> device: 0, name: NVIDIA A30, pci bus id: 0000:12:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "logical_devices = tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BMKbWyXT7kb4"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 200\n",
    "BERT = 'vinai/bertweet-large'\n",
    "N_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NQwWopB77pr1"
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    print(f'reading {path}')\n",
    "    data = pd.read_csv(path)\n",
    "    data.text = data.apply(lambda row: row.text.encode('ascii', 'ignore').decode('ascii').lower(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"removed|deleted\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\" :\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[a-zA-Z]*lt;3[a-zA-Z]*\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[a-zA-Z]&[a-zA-Z]*\", \"\", row.text), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"[^a-zA-Z:.,;'!?\\d]+\", \" \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"i m |im |i'm \", \"i am \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"ive \", \"i have \", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"wasnt|wasn't\", \"was not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"werent|weren't\", \"were not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"dont|don't\", \"do not\", row.text).strip(), 1)\n",
    "    data.text = data.apply(lambda row: re.sub(\"doesnt|doesn't\", \"does not\", row.text).strip(), 1)\n",
    "    texts = data.text.values\n",
    "    labels = data.labels.values\n",
    "    encoder = ohe(sparse=False)\n",
    "    labels = np.array(labels).reshape(-1, 1)\n",
    "    enc_labels = encoder.fit_transform(labels)\n",
    "    print(f'texts shape: {texts.shape}, labels shape: {enc_labels.shape}')\n",
    "    return texts, enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1YkmmDUT8Vga"
   },
   "outputs": [],
   "source": [
    "def prepare_bert_input(sentences, seq_len, bert_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "    encodings = tokenizer(sentences.tolist(), truncation=True, padding='max_length', max_length=seq_len)\n",
    "    if bert_name.startswith(\"roberta\") or \"bertweet\" in bert_name or \"distilbert\" in bert_name:\n",
    "        input = [np.array(encodings[\"input_ids\"]), np.array(encodings[\"attention_mask\"])]\n",
    "    else:\n",
    "        input = [np.array(encodings[\"input_ids\"]), np.array(encodings[\"token_type_ids\"]),\n",
    "               np.array(encodings[\"attention_mask\"])]\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = English()\n",
    "\n",
    "def compute_att_mask(ids):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT)\n",
    "    no_sw_att_mask = []\n",
    "    for enc_sentence in ids:\n",
    "        tokens = [tokenizer.decode([i]) for i in enc_sentence]\n",
    "        lexemes = [nlp.vocab[token.strip()] for token in tokens]\n",
    "        mask = [1 if not (lexeme.is_stop or lexeme.is_punct or len(lexeme.text) <= 2 or \"<\" in lexeme.text) else 0 for lexeme in lexemes]\n",
    "        no_sw_att_mask.append(np.array(mask))\n",
    "    return np.array(no_sw_att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10952,
     "status": "ok",
     "timestamp": 1701817510390,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "i-4Je75_8bNl",
    "outputId": "808f84a8-99e1-4607-82e1-9e030004826d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ../dep-det-data/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts shape: (6006,), labels shape: (6006, 3)\n",
      "reading ../dep-det-data/dev.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts shape: (1000,), labels shape: (1000, 3)\n",
      "reading ../dep-det-data/test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts shape: (3245,), labels shape: (3245, 3)\n"
     ]
    }
   ],
   "source": [
    "sentences_train, labels_train = read_data(\"../dep-det-data/train.csv\")\n",
    "sentences_val, labels_val = read_data(\"../dep-det-data/dev.csv\")\n",
    "sentences_test, labels_test = read_data(\"../dep-det-data/test.csv\")\n",
    "\n",
    "# permutation train\n",
    "perm_train = np.random.permutation(len(sentences_train))\n",
    "sentences_train = sentences_train[perm_train]\n",
    "labels_train = labels_train[perm_train]\n",
    "\n",
    "# permutation val\n",
    "perm_val = np.random.permutation(len(sentences_val))\n",
    "sentences_val = sentences_val[perm_val]\n",
    "labels_val = labels_val[perm_val]\n",
    "\n",
    "# permutation test\n",
    "perm_test = np.random.permutation(len(sentences_test))\n",
    "sentences_test = sentences_test[perm_test]\n",
    "labels_test = labels_test[perm_test]\n",
    "\n",
    "# prepare model input\n",
    "X_train = prepare_bert_input(sentences_train, MAX_SEQ_LEN, BERT)\n",
    "X_val = prepare_bert_input(sentences_val, MAX_SEQ_LEN, BERT)\n",
    "X_test = prepare_bert_input(sentences_test, MAX_SEQ_LEN, BERT)\n",
    "\n",
    "# add custom attention mask\n",
    "mask_train = compute_att_mask(X_train[0])\n",
    "X_train = [X_train[0], X_train[1], mask_train]\n",
    "mask_val = compute_att_mask(X_val[0])\n",
    "X_val = [X_val[0], X_val[1], mask_val]\n",
    "mask_test = compute_att_mask(X_test[0])\n",
    "X_test = [X_test[0], X_test[1], mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 292873,
     "status": "error",
     "timestamp": 1701820263972,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "Ez0L7WSm8cNR",
    "outputId": "31aec781-35ea-4c24-ef06-525f38a0ce46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-02-09 17:04:02.293529: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " roberta_model_input_ids (I  [(None, 200)]                0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " roberta_model_attention_ma  [(None, 200)]                0         []                            \n",
      " sk (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   3543101   ['roberta_model_input_ids[0][0\n",
      " r)                          ngAndCrossAttentions(last_   44        ]',                           \n",
      "                             hidden_state=(None, 200, 1              'roberta_model_attention_mask\n",
      "                             024),                                  [0][0]']                      \n",
      "                              pooler_output=None, past_                                           \n",
      "                             key_values=None, hidden_st                                           \n",
      "                             ates=None, attentions=None                                           \n",
      "                             , cross_attentions=None)                                             \n",
      "                                                                                                  \n",
      " custom_att_mask (InputLaye  [(None, 200)]                0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  [(None, 200, 512),           2623488   ['roberta[0][0]']             \n",
      " al)                          (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " tf.ones_like (TFOpLambda)   (None, 200)                  0         ['custom_att_mask[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 200, 512)             262144    ['bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " subtract (Subtract)         (None, 200)                  0         ['custom_att_mask[0][0]',     \n",
      "                                                                     'tf.ones_like[0][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 200, 1)               512       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 200)                  0         ['subtract[0][0]']            \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 200)                  0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)        (None, 200)                  0         ['tf.math.multiply[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 200)                  0         ['flatten[0][0]',             \n",
      "                                                                     'tf.cast[0][0]']             \n",
      "                                                                                                  \n",
      " alpha (Softmax)             (None, 200)                  0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                   (None, 1024)                 0         ['roberta[0][0]',             \n",
      "                                                                     'alpha[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 3)                    3075      ['dot[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 357199363 (1.33 GB)\n",
      "Trainable params: 357199363 (1.33 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BERT-XDD model initialization\n",
    "roberta_model_input_ids = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_model_input_ids')\n",
    "roberta_model_input_mask = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_model_attention_mask')\n",
    "custom_att_mask = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='custom_att_mask')\n",
    "roberta_model_inputs = [roberta_model_input_ids, roberta_model_input_mask]\n",
    "roberta_model = TFRobertaForSequenceClassification.from_pretrained(BERT, num_labels=N_CLASSES)\n",
    "roberta_model_encoder, roberta_model_classifier, config = roberta_model.roberta, roberta_model.classifier, roberta_model.config\n",
    "\n",
    "encoder_output = roberta_model_encoder(roberta_model_inputs)\n",
    "hidden_state = encoder_output[0]\n",
    "\n",
    "units=256\n",
    "\n",
    "states, forward_h, _, backward_h, _ = layers.Bidirectional(layers.LSTM(units, return_sequences=True, return_state=True))(hidden_state)\n",
    "hidden = layers.Dense(units*2, activation=\"tanh\", use_bias=False)(states)\n",
    "out = layers.Dense(1, activation='linear', use_bias=False)(hidden)\n",
    "energy = layers.Flatten()(out)\n",
    "ones = tf.ones_like(custom_att_mask)\n",
    "att_mask = layers.Subtract()([custom_att_mask, ones])\n",
    "att_mask = att_mask*10000\n",
    "att_mask = tf.cast(att_mask, \"float32\")\n",
    "flat = layers.Add()([energy, att_mask])\n",
    "normalize = layers.Softmax()\n",
    "normalize._init_set_name(\"alpha\")\n",
    "alpha = normalize(flat)\n",
    "ctx = layers.Dot(axes=1)([hidden_state, alpha])\n",
    "pred = layers.Dense(N_CLASSES, activation=\"softmax\")(ctx)\n",
    "\n",
    "BERT_XDD_model = keras.Model(inputs=[roberta_model_input_ids, roberta_model_input_mask, custom_att_mask], outputs=pred)\n",
    "BERT_XDD_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 292873,
     "status": "error",
     "timestamp": 1701820263972,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "Ez0L7WSm8cNR",
    "outputId": "31aec781-35ea-4c24-ef06-525f38a0ce46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load and freeze pre-finetuned encoder's layers\n",
    "\n",
    "# Bertweet pre-finetuned (Roberta architecture)\n",
    "roberta_input_ids = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_input_ids')\n",
    "roberta_input_mask = layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name='roberta_attention_mask')\n",
    "roberta_inputs = [roberta_input_ids, roberta_input_mask]\n",
    "roberta = TFRobertaForSequenceClassification.from_pretrained(BERT, num_labels=N_CLASSES)\n",
    "roberta_output = roberta(roberta_inputs).logits\n",
    "pre_finetuned_model = keras.Model(inputs=roberta_inputs, outputs=roberta_output)\n",
    "\n",
    "pre_finetuned_roberta_input_layer = [pre_finetuned_model.layers[0], pre_finetuned_model.layers[1]]\n",
    "pre_finetuned_roberta_layer = pre_finetuned_model.layers[2]\n",
    "pre_finetuned_encoder, pre_finetuned_classifier = pre_finetuned_roberta_layer.roberta, pre_finetuned_roberta_layer.classifier\n",
    "\n",
    "pre_finetuned_model.load_weights(\"../1_pre-fine-tuning/bertweet.h5\")\n",
    "BERT_XDD_encoder = BERT_XDD_model.layers[2]\n",
    "BERT_XDD_encoder.set_weights(pre_finetuned_encoder.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 292873,
     "status": "error",
     "timestamp": 1701820263972,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "Ez0L7WSm8cNR",
    "outputId": "31aec781-35ea-4c24-ef06-525f38a0ce46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " roberta_model_input_ids (I  [(None, 200)]                0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " roberta_model_attention_ma  [(None, 200)]                0         []                            \n",
      " sk (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   3543101   ['roberta_model_input_ids[0][0\n",
      " r)                          ngAndCrossAttentions(last_   44        ]',                           \n",
      "                             hidden_state=(None, 200, 1              'roberta_model_attention_mask\n",
      "                             024),                                  [0][0]']                      \n",
      "                              pooler_output=None, past_                                           \n",
      "                             key_values=None, hidden_st                                           \n",
      "                             ates=None, attentions=None                                           \n",
      "                             , cross_attentions=None)                                             \n",
      "                                                                                                  \n",
      " custom_att_mask (InputLaye  [(None, 200)]                0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  [(None, 200, 512),           2623488   ['roberta[0][0]']             \n",
      " al)                          (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " tf.ones_like (TFOpLambda)   (None, 200)                  0         ['custom_att_mask[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 200, 512)             262144    ['bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " subtract (Subtract)         (None, 200)                  0         ['custom_att_mask[0][0]',     \n",
      "                                                                     'tf.ones_like[0][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 200, 1)               512       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 200)                  0         ['subtract[0][0]']            \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 200)                  0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)        (None, 200)                  0         ['tf.math.multiply[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 200)                  0         ['flatten[0][0]',             \n",
      "                                                                     'tf.cast[0][0]']             \n",
      "                                                                                                  \n",
      " alpha (Softmax)             (None, 200)                  0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                   (None, 1024)                 0         ['roberta[0][0]',             \n",
      "                                                                     'alpha[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 3)                    3075      ['dot[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 357199363 (1.33 GB)\n",
      "Trainable params: 2889219 (11.02 MB)\n",
      "Non-trainable params: 354310144 (1.32 GB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# train the model's head\n",
    "BERT_XDD_encoder.trainable = False\n",
    "BERT_XDD_model.summary()\n",
    "\n",
    "max_epochs = 6\n",
    "batch_size = 16\n",
    "opt = tf.optimizers.Adam()\n",
    "loss = keras.losses.CategoricalCrossentropy()\n",
    "best_weights_file = f\"BERT-XDD_TL.h5\"\n",
    "acc = keras.metrics.CategoricalAccuracy()\n",
    "f1_macro = keras.metrics.F1Score(average='macro')\n",
    "m_ckpt = ModelCheckpoint(best_weights_file, monitor='val_'+f1_macro.name, mode='max', verbose=2,\n",
    "                          save_weights_only=True, save_best_only=True)\n",
    "\n",
    "BERT_XDD_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "\n",
    "### uncomment to train model's head ###\n",
    "\n",
    "# BERT_XDD_model.fit(\n",
    "# X_train, labels_train,\n",
    "# validation_data=(X_val, labels_val),\n",
    "# epochs=max_epochs,\n",
    "# batch_size=batch_size,\n",
    "# callbacks=[m_ckpt],\n",
    "# verbose=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24891,
     "status": "ok",
     "timestamp": 1701820290757,
     "user": {
      "displayName": "Riccardo Cantini",
      "userId": "12249471593588897634"
     },
     "user_tz": -60
    },
    "id": "wtVgPnsx8kPN",
    "outputId": "1d1085f5-9737-488b-ae98-781c27fbd6eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/102 [..............................] - ETA: 14:49"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 17:04:23.967660: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 67s 579ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.433     0.557     0.488       228\n",
      "           1      0.781     0.631     0.698      2169\n",
      "           2      0.435     0.614     0.509       848\n",
      "\n",
      "    accuracy                          0.622      3245\n",
      "   macro avg      0.550     0.601     0.565      3245\n",
      "weighted avg      0.666     0.622     0.634      3245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# test the model\n",
    "\n",
    "best_weights_file = f\"BERT-XDD_TL.h5\"\n",
    "BERT_XDD_model.load_weights(best_weights_file)\n",
    "\n",
    "BERT_XDD_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "y_pred_probs = BERT_XDD_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "labels_test_decode = np.argmax(labels_test, axis=1)\n",
    "report = classification_report(labels_test_decode, y_pred, digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " roberta_model_input_ids (I  [(None, 200)]                0         []                            \n",
      " nputLayer)                                                                                       \n",
      "                                                                                                  \n",
      " roberta_model_attention_ma  [(None, 200)]                0         []                            \n",
      " sk (InputLayer)                                                                                  \n",
      "                                                                                                  \n",
      " roberta (TFRobertaMainLaye  TFBaseModelOutputWithPooli   3543101   ['roberta_model_input_ids[0][0\n",
      " r)                          ngAndCrossAttentions(last_   44        ]',                           \n",
      "                             hidden_state=(None, 200, 1              'roberta_model_attention_mask\n",
      "                             024),                                  [0][0]']                      \n",
      "                              pooler_output=None, past_                                           \n",
      "                             key_values=None, hidden_st                                           \n",
      "                             ates=None, attentions=None                                           \n",
      "                             , cross_attentions=None)                                             \n",
      "                                                                                                  \n",
      " custom_att_mask (InputLaye  [(None, 200)]                0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  [(None, 200, 512),           2623488   ['roberta[0][0]']             \n",
      " al)                          (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " tf.ones_like (TFOpLambda)   (None, 200)                  0         ['custom_att_mask[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 200, 512)             262144    ['bidirectional[0][0]']       \n",
      "                                                                                                  \n",
      " subtract (Subtract)         (None, 200)                  0         ['custom_att_mask[0][0]',     \n",
      "                                                                     'tf.ones_like[0][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 200, 1)               512       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 200)                  0         ['subtract[0][0]']            \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 200)                  0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.cast (TFOpLambda)        (None, 200)                  0         ['tf.math.multiply[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 200)                  0         ['flatten[0][0]',             \n",
      "                                                                     'tf.cast[0][0]']             \n",
      "                                                                                                  \n",
      " alpha (Softmax)             (None, 200)                  0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                   (None, 1024)                 0         ['roberta[0][0]',             \n",
      "                                                                     'alpha[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 3)                    3075      ['dot[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 357199363 (1.33 GB)\n",
      "Trainable params: 357199363 (1.33 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## end-to-end fine-tuning\n",
    "\n",
    "best_weights_file = f\"BERT-XDD_TL.h5\"\n",
    "BERT_XDD_model.load_weights(best_weights_file)\n",
    "BERT_XDD_model.trainable = True\n",
    "\n",
    "max_epochs = 2\n",
    "batch_size = 16\n",
    "opt = tfa.optimizers.RectifiedAdam(learning_rate=3e-5)\n",
    "loss = keras.losses.CategoricalCrossentropy()\n",
    "best_weights_file = f\"BERT-XDD_FT.h5\"\n",
    "acc = keras.metrics.CategoricalAccuracy()\n",
    "f1_macro = keras.metrics.F1Score(average='macro')\n",
    "m_ckpt = ModelCheckpoint(best_weights_file, monitor='val_'+f1_macro.name, mode='max', verbose=2,\n",
    "                          save_weights_only=True, save_best_only=True)\n",
    "\n",
    "BERT_XDD_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "BERT_XDD_model.summary()\n",
    "\n",
    "### uncomment to perform the end-to-end fine-tuning step ###\n",
    "\n",
    "# BERT_XDD_model.fit(\n",
    "# X_train, labels_train,\n",
    "# validation_data=(X_val, labels_val),\n",
    "# epochs=max_epochs,\n",
    "# batch_size=batch_size,\n",
    "# callbacks=[m_ckpt],\n",
    "# verbose=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 67s 585ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.498     0.535     0.516       228\n",
      "           1      0.771     0.722     0.745      2169\n",
      "           2      0.478     0.546     0.510       848\n",
      "\n",
      "    accuracy                          0.663      3245\n",
      "   macro avg      0.582     0.601     0.590      3245\n",
      "weighted avg      0.675     0.663     0.668      3245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "from sklearn.metrics import classification_report\n",
    "best_weights_file = f\"BERT-XDD_FT.h5\"\n",
    "BERT_XDD_model.load_weights(best_weights_file)\n",
    "BERT_XDD_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "y_pred_probs = BERT_XDD_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "labels_test_decode = np.argmax(labels_test, axis=1)\n",
    "\n",
    "report = classification_report(labels_test_decode, y_pred, digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 67s 586ms/step\n"
     ]
    }
   ],
   "source": [
    "# create the attention inspection model (for explainability purposes)\n",
    "best_weights_file = f\"BERT-XDD_FT.h5\"\n",
    "BERT_XDD_model.load_weights(best_weights_file)\n",
    "BERT_XDD_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "weight_model = keras.Model(\n",
    "    inputs=[roberta_model_input_ids, roberta_model_input_mask, custom_att_mask],\n",
    "    outputs=BERT_XDD_model.get_layer(\"alpha\").output)\n",
    "# restore weights\n",
    "for l1, l2 in zip(weight_model.layers, BERT_XDD_model.layers):\n",
    "    l1.set_weights(l2.get_weights())\n",
    "weight_model.compile(loss=loss, optimizer=opt, metrics=[f1_macro,acc])\n",
    "\n",
    "out_p = weight_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_name(arr):\n",
    "    n = {0:\"SEVERE_DEP\", 1: \"DEP\", 2: \"NORMAL\"}\n",
    "    return n[np.argmax(arr)]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT)\n",
    "def get_word_weights(id, top_n=200):\n",
    "    ids_test = X_test[0]\n",
    "    tokens = [tokenizer.decode([i]) for i in ids_test[id]]\n",
    "    d = {}\n",
    "    for token, weight in zip(tokens, out_p[id]):\n",
    "        weight = weight\n",
    "        if token not in d:\n",
    "            d[token]=weight\n",
    "        else:\n",
    "            d[token] = max(d[token], weight)\n",
    "    d_sorted = dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:top_n])\n",
    "    in_sentence = tokenizer.decode(ids_test[id])\n",
    "    end = in_sentence.index(\"</s>\")\n",
    "    return {\"sentence\": in_sentence[:end],\n",
    "            \"pred_label\":get_label_name(y_pred_probs[id]),\n",
    "            \"real_label\":get_label_name(labels_test[id]),\n",
    "            \"weights\": d_sorted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': '<s>at the doctor... getting refills on pain meds for my shitty back. already made to feel like a drug addict despite taking only 1 pill per day.... anyway...the nurse chomping her gum in my ear and clickety clacking her laptop keys with her bedazzled nails squints her fake tanned eyes over her reading glasses and blurts it says i have to ask some screener questions have you lost interest in things you normally enjoy? every day, sometimes, once or twice, or never? in my head : yes. i do not have any desire to do anything. nothing sounds fun. if i didnt have responsibilities id likely spend days in my bed. out loud : nah. in the past month have you felt that your life was out of control or have you felt overwhelmed? every day, sometimes, once or twice, never? in my head : constantly our loud : never ok last one, do you ever feel hopeless or sad for no reason', 'pred_label': 'DEP', 'real_label': 'DEP', 'weights': {' pill': 0.0482861, ' despite': 0.047813658, ' taking': 0.04728498, ' nurse': 0.038014106, ' doctor': 0.03452121, ' day': 0.033861157, ' getting': 0.03191509, ' drug': 0.031152027, ' loud': 0.03072872, ' addict': 0.030377928, ' med': 0.028322672, ' pain': 0.027815584, ' shitty': 0.027190913, 'ills': 0.026636548, ' gum': 0.025728734, ' like': 0.02352765, 'ety': 0.021743065, ' head': 0.02007826, 'acking': 0.019331679, ' enjoy': 0.01901982, ' things': 0.017918732, ' bed': 0.016922189, ' says': 0.016379729, ' yes': 0.01631516, 'ing': 0.015346244, ' ref': 0.01490805, ' feel': 0.013156042, ' questions': 0.01289866, ' blur': 0.0127380155, 'omp': 0.012274377, ' click': 0.011415804, 'ener': 0.011266776, ' reason': 0.008973422, ' month': 0.00881336, ' overwhelmed': 0.008665965, ' constantly': 0.008186219, ' responsibilities': 0.008059663, ' hopeless': 0.007857278, ' life': 0.0062866337, ' sad': 0.005320005, ' likely': 0.0047754347, ' ear': 0.0046637543, 'azz': 0.0043374477, ' interest': 0.0042932183, 'led': 0.0040914635, ' spend': 0.0039899964, ' sounds': 0.0036936945, ' fun': 0.003652515, ' nails': 0.0034537327, ' control': 0.003447009, ' laptop': 0.003399688, ' felt': 0.003328341, ' didnt': 0.003206127, ' ask': 0.0029575767, ' past': 0.0027174358, ' normally': 0.0026571876, ' twice': 0.00250294, ' lost': 0.0024191586, ' fake': 0.002226647, ' keys': 0.0021896292, ' days': 0.0021829708, ' squ': 0.0015297189, 'anned': 0.0009386917, 'ints': 0.0008818699, ' desire': 0.0008689488, ' reading': 0.0007532722, ' glasses': 0.00036975057, ' eyes': 0.0002610888, '<s>': 0.0, 'at': 0.0, ' the': 0.0, '...': 0.0, ' on': 0.0, 's': 0.0, ' for': 0.0, ' my': 0.0, ' back': 0.0, '.': 0.0, ' already': 0.0, ' made': 0.0, ' to': 0.0, ' a': 0.0, ' only': 0.0, ' 1': 0.0, ' per': 0.0, '....': 0.0, ' anyway': 0.0, 'the': 0.0, ' ch': 0.0, ' her': 0.0, ' in': 0.0, ' and': 0.0, ' cl': 0.0, ' with': 0.0, ' t': 0.0, ' over': 0.0, 'ts': 0.0, ' it': 0.0, ' i': 0.0, ' have': 0.0, ' some': 0.0, ' sc': 0.0, 're': 0.0, ' you': 0.0, '?': 0.0, ' every': 0.0, ',': 0.0, ' sometimes': 0.0, ' once': 0.0, ' or': 0.0, ' never': 0.0, ' :': 0.0, ' do': 0.0, ' not': 0.0, ' any': 0.0, ' anything': 0.0, ' nothing': 0.0, ' if': 0.0, ' id': 0.0, ' out': 0.0, ' n': 0.0, 'ah': 0.0, ' that': 0.0, ' your': 0.0, ' was': 0.0, ' of': 0.0, ' our': 0.0, ' ok': 0.0, ' last': 0.0, ' one': 0.0, ' ever': 0.0, ' no': 0.0, '</s>': 0.0}} \n",
      "\n",
      "{'sentence': '<s>nothing i achieve makes me happy. i do not understand how why when i achieve something, the happiness i feel is so minimal yet when i mess something up, i feel so much sadness and self hatred. the happiness i felt from getting into one of my top colleges was only felt for one instant before i slipped back. i cant ever seem to be proud of myself, no matter how much others are. the pain just never goes away and i want to be dead.', 'pred_label': 'DEP', 'real_label': 'DEP', 'weights': {' makes': 0.089891955, ' happy': 0.077785045, ' dead': 0.077745244, ' achieve': 0.07253292, ' minimal': 0.06426822, ' cant': 0.061432075, ' want': 0.047773078, ' happiness': 0.041522853, ' understand': 0.04102792, ' pain': 0.03330835, ' goes': 0.032533765, ' proud': 0.030806351, ' feel': 0.030282255, ' away': 0.02991354, ' hatred': 0.028807625, ' slipped': 0.028229047, ' sadness': 0.025833314, ' self': 0.019368194, ' matter': 0.0179971, ' mess': 0.016578102, ' instant': 0.01631364, ' felt': 0.01289846, ' colleges': 0.010788395, ' getting': 0.0066487812, '<s>': 0.0, 'nothing': 0.0, ' i': 0.0, ' me': 0.0, '.': 0.0, ' do': 0.0, ' not': 0.0, ' how': 0.0, ' why': 0.0, ' when': 0.0, ' something': 0.0, ',': 0.0, ' the': 0.0, ' is': 0.0, ' so': 0.0, ' yet': 0.0, ' up': 0.0, ' much': 0.0, ' and': 0.0, ' from': 0.0, ' into': 0.0, ' one': 0.0, ' of': 0.0, ' my': 0.0, ' top': 0.0, ' was': 0.0, ' only': 0.0, ' for': 0.0, ' before': 0.0, ' back': 0.0, ' ever': 0.0, ' seem': 0.0, ' to': 0.0, ' be': 0.0, ' myself': 0.0, ' no': 0.0, ' others': 0.0, ' are': 0.0, ' just': 0.0, ' never': 0.0, '</s>': 0.0, '<pad>': 0.0}} \n",
      "\n",
      "{'sentence': \"<s>i've got no aspiration, no interest in anything, no fun why do i feel like it takes so much effort to do anything in life. a little background about me: i am a college student. i've been living a decent life with not much trouble in life. recently i figured out that i might be depressed. i have so much trouble making friends ever since a few years ago. when i meet someone for the first time i try to look cheerful but it gets very tiring and i realized that i avoid gatherings because i find it pointless and tiring. at this point i am too scared to ask how to make friends nowadays. i hate social media because it makes me sad to see all other people having a blast of life in their lives. i've got no interest in life or have a hobby that i am geniunely interested in. i find this espiecially true with my college work. i deal well with doing things that i am told to but i\", 'pred_label': 'DEP', 'real_label': 'DEP', 'weights': {' depressed': 0.038028307, ' nowadays': 0.034121506, ' got': 0.033048067, ' trouble': 0.032864552, ' aspiration': 0.032765478, ' recently': 0.03186891, ' life': 0.0317999, ' fun': 0.03023764, ' background': 0.028963398, ' hate': 0.028659543, ' gatherings': 0.02829159, ' social': 0.024891863, ' media': 0.024029287, ' decent': 0.021698814, ' friends': 0.018748183, ' student': 0.01709931, ' effort': 0.01703917, ' living': 0.015505818, ' feel': 0.015495489, ' like': 0.015131171, ' interest': 0.014678135, ' takes': 0.014337701, ' work': 0.0139905885, ' college': 0.013612723, ' figured': 0.013564132, ' little': 0.013486516, ' cheerful': 0.013035666, ' makes': 0.012905086, ' pointless': 0.012772759, ' ago': 0.012754085, ' find': 0.011986438, ' avoid': 0.011564887, ' making': 0.011429585, ' point': 0.0113762915, ' scared': 0.0107572675, ' sad': 0.009867806, 'iring': 0.0095749, ' hobby': 0.009269851, ' blast': 0.008342932, ' lives': 0.0081873685, ' ask': 0.00808581, ' years': 0.0075048828, ' having': 0.0068020467, ' realized': 0.006618917, ' people': 0.006128027, ' meet': 0.0056665717, ' true': 0.0054578637, ' things': 0.0053667803, ' esp': 0.004942902, ' gets': 0.0045663198, ' told': 0.004310316, ' interested': 0.004231841, 'une': 0.0035908723, ' time': 0.0030233306, ' gen': 0.0027413804, ' try': 0.002730101, ' deal': 0.002674068, 'cially': 0.0024222129, ' look': 0.0017695029, '<s>': 0.0, 'i': 0.0, \"'ve\": 0.0, ' no': 0.0, ',': 0.0, ' in': 0.0, ' anything': 0.0, ' why': 0.0, ' do': 0.0, ' i': 0.0, ' it': 0.0, ' so': 0.0, ' much': 0.0, ' to': 0.0, '.': 0.0, ' a': 0.0, ' about': 0.0, ' me': 0.0, ':': 0.0, ' am': 0.0, ' been': 0.0, ' with': 0.0, ' not': 0.0, ' out': 0.0, ' that': 0.0, ' might': 0.0, ' be': 0.0, ' have': 0.0, ' ever': 0.0, ' since': 0.0, ' few': 0.0, ' when': 0.0, ' someone': 0.0, ' for': 0.0, ' the': 0.0, ' first': 0.0, ' but': 0.0, ' very': 0.0, ' t': 0.0, ' and': 0.0, ' because': 0.0, ' at': 0.0, ' this': 0.0, ' too': 0.0, ' how': 0.0, ' make': 0.0, ' see': 0.0, ' all': 0.0, ' other': 0.0, ' of': 0.0, ' their': 0.0, ' or': 0.0, 'ly': 0.0, 'ie': 0.0, ' my': 0.0, ' well': 0.0, ' doing': 0.0, '</s>': 0.0}} \n",
      "\n",
      "{'sentence': '<s>nothing i ever do is good enough everyone consistently sets high standards for me in my career and when i finally achieve them i am rewarded with nothing but higher goals while the fruits of my labor get passed to upper management. my girlfriend cheated on me because i guess i was not enough. my younger sister graduated college first and i am just now graduating from a community college because i was not good enough to go after high school. i honestly just wanna gi have up but i am too much of a coward to take myself out.', 'pred_label': 'DEP', 'real_label': 'NORMAL', 'weights': {' career': 0.13047086, ' consistently': 0.10767368, ' wanna': 0.09307878, ' coward': 0.07470646, ' rewarded': 0.05905144, ' good': 0.05631805, ' honestly': 0.03772879, ' management': 0.033337597, ' upper': 0.029786946, ' sets': 0.02616648, ' achieve': 0.025528817, ' fruits': 0.023459619, ' girlfriend': 0.023445433, ' goals': 0.023058485, ' standards': 0.020636054, ' high': 0.020512506, ' cheated': 0.019804442, ' graduated': 0.019366816, ' finally': 0.016801361, ' passed': 0.016628718, ' graduating': 0.015813738, ' higher': 0.0150591945, ' community': 0.012560055, ' guess': 0.012420085, ' sister': 0.012259058, ' school': 0.011857545, ' college': 0.010711015, ' labor': 0.008498909, ' younger': 0.0070056277, '<s>': 0.0, 'nothing': 0.0, ' i': 0.0, ' ever': 0.0, ' do': 0.0, ' is': 0.0, ' enough': 0.0, ' everyone': 0.0, ' for': 0.0, ' me': 0.0, ' in': 0.0, ' my': 0.0, ' and': 0.0, ' when': 0.0, ' them': 0.0, ' am': 0.0, ' with': 0.0, ' nothing': 0.0, ' but': 0.0, ' while': 0.0, ' the': 0.0, ' of': 0.0, ' get': 0.0, ' to': 0.0, '.': 0.0, ' on': 0.0, ' because': 0.0, ' was': 0.0, ' not': 0.0, ' first': 0.0, ' just': 0.0, ' now': 0.0, ' from': 0.0, ' a': 0.0, ' go': 0.0, ' after': 0.0, ' g': 0.0, 'i': 0.0, ' have': 0.0, ' up': 0.0, ' too': 0.0, ' much': 0.0, ' take': 0.0, ' myself': 0.0, ' out': 0.0, '</s>': 0.0, '<pad>': 0.0}} \n",
      "\n",
      "{'sentence': '<s>i hate this fuckin world yall can suck my dick', 'pred_label': 'DEP', 'real_label': 'NORMAL', 'weights': {' fuckin': 0.26540756, ' dick': 0.20584336, ' hate': 0.20154269, ' world': 0.19304372, ' suck': 0.13416268, '<s>': 0.0, 'i': 0.0, ' this': 0.0, ' y': 0.0, 'all': 0.0, ' can': 0.0, ' my': 0.0, '</s>': 0.0, '<pad>': 0.0}} \n",
      "\n",
      "{'sentence': '<s>i do not even know what happened literally i was out with a friend and their friends and they were having a jam session playing guitar and everything and suddenly everything seemed so distant. my chest got really tight and the world felt like it was shrinking around me. my breathe started getting short and i had to leave immediately. she asked if i was okay and i couldnt respond. i finally got some words out and she tried to kiss me goodbye but i just couldnt stand still. i basically ran to the other side of my car and wished her a good night and i left. i texted her and apologized and she seems to understand but i just feel so gross. i was probably on the verge of a panic attack but i do not know what triggered it', 'pred_label': 'NORMAL', 'real_label': 'DEP', 'weights': {' gross': 0.086926825, ' distant': 0.07088926, ' literally': 0.064793125, ' happened': 0.052284982, ' got': 0.03372441, ' friend': 0.031701937, ' suddenly': 0.03062342, ' world': 0.029275967, ' know': 0.027816014, ' triggered': 0.02541289, ' immediately': 0.025186436, ' shrinking': 0.023355268, ' tight': 0.021143112, ' like': 0.018781139, ' panic': 0.018655144, ' car': 0.018325284, ' okay': 0.017534448, ' left': 0.01727306, ' breathe': 0.01643659, ' chest': 0.01592962, ' started': 0.015889982, ' getting': 0.015710397, ' jam': 0.015057515, ' kiss': 0.01489262, ' friends': 0.0134641435, ' guitar': 0.01337003, ' probably': 0.012915278, ' having': 0.012605969, ' leave': 0.012525694, ' felt': 0.01234882, ' attack': 0.012200608, ' wished': 0.012199941, ' verge': 0.012096008, ' playing': 0.012047988, ' session': 0.011594328, ' basically': 0.011097151, ' ran': 0.01107224, ' short': 0.010585798, ' feel': 0.01043875, ' good': 0.010325384, ' texted': 0.009551264, ' respond': 0.008917849, ' stand': 0.008342476, ' words': 0.0077488814, ' apologized': 0.007722213, ' night': 0.0076475767, ' goodbye': 0.006098333, ' asked': 0.0058046402, ' understand': 0.004543763, ' finally': 0.0039336514, ' tried': 0.0020513688, '<s>': 0.0, 'i': 0.0, ' do': 0.0, ' not': 0.0, ' even': 0.0, ' what': 0.0, ' i': 0.0, ' was': 0.0, ' out': 0.0, ' with': 0.0, ' a': 0.0, ' and': 0.0, ' their': 0.0, ' they': 0.0, ' were': 0.0, ' everything': 0.0, ' seemed': 0.0, ' so': 0.0, '.': 0.0, ' my': 0.0, ' really': 0.0, ' the': 0.0, ' it': 0.0, ' around': 0.0, ' me': 0.0, ' had': 0.0, ' to': 0.0, ' she': 0.0, ' if': 0.0, ' could': 0.0, 'nt': 0.0, ' some': 0.0, ' but': 0.0, ' just': 0.0, ' still': 0.0, ' other': 0.0, ' side': 0.0, ' of': 0.0, ' her': 0.0, ' seems': 0.0, ' on': 0.0, '</s>': 0.0, '<pad>': 0.0}} \n",
      "\n",
      "{'sentence': '<s>i cannot stop thinking about suicide i have persistent suicidal ideation, but as time passes i am realizing that these thoughts are actually giving me what little comfort i have. these thoughts are in constant conflict of reminding how miserable i am while simultaneously giving me the comfort of having some modicum of control. at least i have the ability to go out on my own terms if nothing else. i am not actively planning to do it or anything, but its at the point where i am uncomfortable if its not on my mind. i am so tired of feeling this way', 'pred_label': 'DEP', 'real_label': 'SEVERE_DEP', 'weights': {' uncomfortable': 0.048467625, ' comfort': 0.046444107, ' actually': 0.03512716, ' giving': 0.034984343, ' thinking': 0.033658102, ' ide': 0.03328018, ' conflict': 0.03306379, ' realizing': 0.032709174, ' control': 0.032709066, ' persistent': 0.031093417, 'ation': 0.031048749, ' suicide': 0.030816065, ' stop': 0.030635549, ' way': 0.030217491, ' suicidal': 0.030099027, ' thoughts': 0.029414093, ' little': 0.026897205, ' time': 0.026283694, ' feeling': 0.025693817, ' constant': 0.025621923, ' passes': 0.024999727, ' tired': 0.02340613, ' miserable': 0.022368513, ' ability': 0.021965796, ' point': 0.021884913, ' mind': 0.021496013, ' terms': 0.020802611, ' simultaneously': 0.019846799, ' actively': 0.019629333, 'icum': 0.018759917, ' having': 0.017625585, ' reminding': 0.017425438, ' planning': 0.017369075, ' mod': 0.008525545, '<s>': 0.0, 'i': 0.0, ' cannot': 0.0, ' about': 0.0, ' i': 0.0, ' have': 0.0, ',': 0.0, ' but': 0.0, ' as': 0.0, ' am': 0.0, ' that': 0.0, ' these': 0.0, ' are': 0.0, ' me': 0.0, ' what': 0.0, '.': 0.0, ' in': 0.0, ' of': 0.0, ' how': 0.0, ' while': 0.0, ' the': 0.0, ' some': 0.0, ' at': 0.0, ' least': 0.0, ' to': 0.0, ' go': 0.0, ' out': 0.0, ' on': 0.0, ' my': 0.0, ' own': 0.0, ' if': 0.0, ' nothing': 0.0, ' else': 0.0, ' not': 0.0, ' do': 0.0, ' it': 0.0, ' or': 0.0, ' anything': 0.0, ' its': 0.0, ' where': 0.0, ' so': 0.0, ' this': 0.0, '</s>': 0.0, '<pad>': 0.0}} \n",
      "\n",
      "{'sentence': '<s>it finally happened again been talking with a girl, and shes really nice. i thought we were making a connection but its happening again. the same thing that happens every time no matter who i talk to. shes receding. we were having conversations with paragraph long texts. i really tried to be careful, and not scare her away, but i do not know why i expected anything different. honestly the thought of anyone ever loving me, is so far fetched, and sickening at this point. i do not blame her. i wish i could just die. my life was just a bottomless pit that i threw everything i had into.', 'pred_label': 'NORMAL', 'real_label': 'DEP', 'weights': {' honestly': 0.09316601, ' life': 0.07160579, 'eding': 0.06362505, ' rec': 0.05004122, ' die': 0.044070356, ' happened': 0.042469013, ' thing': 0.04212979, ' finally': 0.039247107, ' happening': 0.032592844, ' pit': 0.032514118, ' talking': 0.032092966, ' girl': 0.031277254, ' wish': 0.028871985, ' talk': 0.02849051, ' happens': 0.02542054, ' point': 0.023942938, ' paragraph': 0.023854258, ' sick': 0.023476128, ' nice': 0.022281278, ' loving': 0.02201787, 'ening': 0.021001102, ' time': 0.016772784, ' threw': 0.016673295, ' matter': 0.015421864, ' away': 0.015241354, ' expected': 0.015180958, ' different': 0.014300124, ' thought': 0.013003825, ' connection': 0.009972553, ' scare': 0.009173302, ' know': 0.008749545, ' texts': 0.008518174, ' making': 0.0077043287, ' careful': 0.006857734, ' far': 0.006529126, ' having': 0.0063848346, ' long': 0.0060306513, ' blame': 0.005926427, ' tried': 0.0053119967, 'ched': 0.0044644494, ' conversations': 0.004084576, ' fet': 0.002050046, '<s>': 0.0, 'it': 0.0, ' again': 0.0, ' been': 0.0, ' with': 0.0, ' a': 0.0, ',': 0.0, ' and': 0.0, ' she': 0.0, 's': 0.0, ' really': 0.0, '.': 0.0, ' i': 0.0, ' we': 0.0, ' were': 0.0, ' but': 0.0, ' its': 0.0, ' the': 0.0, ' same': 0.0, ' that': 0.0, ' every': 0.0, ' no': 0.0, ' who': 0.0, ' to': 0.0, ' be': 0.0, ' not': 0.0, ' her': 0.0, ' do': 0.0, ' why': 0.0, ' anything': 0.0, ' of': 0.0, ' anyone': 0.0, ' ever': 0.0, ' me': 0.0, ' is': 0.0, ' so': 0.0, ' at': 0.0, ' this': 0.0, ' could': 0.0, ' just': 0.0, ' my': 0.0, ' was': 0.0, ' bottom': 0.0, 'less': 0.0, ' everything': 0.0, ' had': 0.0, ' into': 0.0, '</s>': 0.0, '<pad>': 0.0}} \n",
      "\n",
      "{'sentence': '<s>: i do not know what to do anymore. i started harming again. i still do not want to be here anymore. i feel selfish, like really guilty and upset with myself. everyday is the same and i am always tired. i do not want to tell my teacher again because then shell have to let my parents know. at the moment my parents are already hella stressed and busy, i am the last thing they need to worry about rn.. i just want it to be over', 'pred_label': 'SEVERE_DEP', 'real_label': 'SEVERE_DEP', 'weights': {' tired': 0.053131767, ' want': 0.051977076, ' thing': 0.049659226, ' harming': 0.049584717, ' everyday': 0.047860358, ' anymore': 0.04475908, ' teacher': 0.04468882, ' know': 0.0444621, ' started': 0.04261618, ' shell': 0.0377604, ' upset': 0.036292594, ' selfish': 0.031623915, ' moment': 0.031234508, ' parents': 0.030398643, ' tell': 0.030141445, ' like': 0.029878972, ' feel': 0.029493863, ' guilty': 0.027066814, ' hell': 0.024038631, ' worry': 0.019686896, ' busy': 0.019264547, ' let': 0.018364396, ' stressed': 0.017934391, ' need': 0.013332724, '<s>': 0.0, ':': 0.0, ' i': 0.0, ' do': 0.0, ' not': 0.0, ' what': 0.0, ' to': 0.0, '.': 0.0, ' again': 0.0, ' still': 0.0, ' be': 0.0, ' here': 0.0, ',': 0.0, ' really': 0.0, ' and': 0.0, ' with': 0.0, ' myself': 0.0, ' is': 0.0, ' the': 0.0, ' same': 0.0, ' am': 0.0, ' always': 0.0, ' my': 0.0, ' because': 0.0, ' then': 0.0, ' have': 0.0, ' at': 0.0, ' are': 0.0, ' already': 0.0, 'a': 0.0, ' last': 0.0, ' they': 0.0, ' about': 0.0, ' r': 0.0, 'n': 0.0, '..': 0.0, ' just': 0.0, ' it': 0.0, ' over': 0.0, '</s>': 0.0, '<pad>': 0.0}} \n",
      "\n",
      "{'sentence': '<s>goodnight, i hope i do not wake up tomorrow. reached an all time low', 'pred_label': 'DEP', 'real_label': 'DEP', 'weights': {' reached': 0.28061175, ' low': 0.18049598, 'night': 0.15277381, 'good': 0.09707794, ' tomorrow': 0.095607124, ' hope': 0.09209343, ' time': 0.06705985, ' wake': 0.034280166, '<s>': 0.0, ',': 0.0, ' i': 0.0, ' do': 0.0, ' not': 0.0, ' up': 0.0, '.': 0.0, ' an': 0.0, ' all': 0.0, '</s>': 0.0, '<pad>': 0.0}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for test_id in range(10): # example expl\n",
    "    print(get_word_weights(test_id), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPwYJoECmZY3TsCeI3Ln81/",
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
